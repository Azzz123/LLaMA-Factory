import json
import os
import argparse
from tqdm import tqdm
from collections import Counter
import pandas as pd


# --- è¾…åŠ©å‡½æ•° ---

def is_valid_json_list(s):
    """æ£€æŸ¥ä¸€ä¸ªå­—ç¬¦ä¸²æ˜¯å¦æ˜¯åˆæ³•çš„JSONåˆ—è¡¨ã€‚"""
    s = s.strip()
    if not s.startswith('[') or not s.endswith(']'):
        return False
    try:
        json.loads(s)
        return True
    except json.JSONDecodeError:
        return False


# --- ä¸»æ£€æŸ¥é€»è¾‘ ---

def inspect_dataset(file_path):
    """
    å¯¹æŒ‡å®šçš„æ•°æ®é›†æ–‡ä»¶è¿›è¡Œå…¨é¢çš„å¥åº·æ£€æŸ¥ã€‚
    """
    print(f"ğŸ©º å¼€å§‹å¯¹æ•°æ®é›†è¿›è¡Œå¥åº·ä½“æ£€: {file_path}\n")

    # --- 1. åˆå§‹åŒ–æŠ¥å‘Šå’Œè®¡æ•°å™¨ ---
    report = {
        "file_path": file_path,
        "total_samples": 0,
        "error_counts": Counter(),
        "error_samples": [],
        "statistics": {
            "event_type_distribution": Counter(),
            "samples_with_no_events": 0,
            "output_token_lengths": []
        }
    }

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        report["total_samples"] = len(data)
    except Exception as e:
        print(f"âŒ è‡´å‘½é”™è¯¯: æ–‡ä»¶æ— æ³•ä½œä¸ºJSONåŠ è½½ã€‚é”™è¯¯: {e}")
        return

    # --- 2. é€æ¡æ ·æœ¬è¿›è¡Œæ£€æŸ¥ ---
    for i, sample in enumerate(tqdm(data, desc="ğŸ” æ£€æŸ¥æ ·æœ¬ä¸­")):
        errors_in_sample = []

        # 2.1 åŸºç¡€æ ¼å¼æ ¡éªŒ
        required_keys = ["instruction", "input", "output", "system"]
        missing_keys = [key for key in required_keys if key not in sample]
        if missing_keys:
            errors_in_sample.append(f"ç¼ºå°‘å…³é”®å­—æ®µ: {', '.join(missing_keys)}")
            report["error_counts"]["missing_keys"] += 1

        output = sample.get("output", "")

        # 2.2 CoTæ ¼å¼ä¸“é¡¹æ ¡éªŒ
        if "<think>" not in output or "</think>" not in output:
            errors_in_sample.append("CoTæ ¼å¼é”™è¯¯: ç¼ºå°‘<think>æˆ–</think>æ ‡ç­¾")
            report["error_counts"]["missing_think_tags"] += 1

        if "</think>\n\n" not in output:
            errors_in_sample.append("CoTæ ¼å¼é”™è¯¯: </think>åç¼ºå°‘'\\n\\n'åˆ†éš”ç¬¦")
            report["error_counts"]["missing_separator"] += 1

        try:
            parts = output.split("</think>\n\n")
            if len(parts) != 2:
                raise ValueError("åˆ†éš”ç¬¦æ•°é‡ä¸ä¸º1")

            think_part, json_part = parts

            if not is_valid_json_list(json_part):
                errors_in_sample.append("JSONç­”æ¡ˆé”™è¯¯: åˆ†éš”ç¬¦åçš„å†…å®¹ä¸æ˜¯ä¸€ä¸ªåˆæ³•çš„JSONåˆ—è¡¨")
                report["error_counts"]["invalid_json_answer"] += 1
            else:
                # 2.3 å†…å®¹é€»è¾‘ä¸€è‡´æ€§æ ¡éªŒ & ç»Ÿè®¡
                json_answer = json.loads(json_part)

                # ç»Ÿè®¡äº‹ä»¶ç±»å‹åˆ†å¸ƒ
                if not json_answer:
                    report["statistics"]["samples_with_no_events"] += 1
                    if "æœªå‘ç°" not in think_part:
                        errors_in_sample.append("é€»è¾‘ä¸ä¸€è‡´: ç­”æ¡ˆä¸ºç©ºåˆ—è¡¨ï¼Œä½†æ€ç»´é“¾æœªæåŠ'æœªå‘ç°'")
                        report["error_counts"]["logic_empty_mismatch"] += 1
                else:
                    for event in json_answer:
                        event_type = event.get("event_type")
                        if event_type:
                            report["statistics"]["event_type_distribution"][event_type] += 1

                    num_events_in_json = len(json_answer)
                    # ç®€å•åœ°é€šè¿‡'äº‹ä»¶:'å…³é”®è¯æ¥ä¼°è®¡æ€ç»´é“¾ä¸­çš„äº‹ä»¶æ•°
                    num_events_in_think = think_part.count("- äº‹ä»¶:")
                    if num_events_in_think != num_events_in_json and num_events_in_think > 0:
                        errors_in_sample.append(
                            f"é€»è¾‘ä¸ä¸€è‡´: æ€ç»´é“¾ä¸­äº‹ä»¶æ•°({num_events_in_think})ä¸JSONç­”æ¡ˆæ•°({num_events_in_json})ä¸åŒ¹é…")
                        report["error_counts"]["logic_count_mismatch"] += 1

        except Exception as e:
            errors_in_sample.append(f"CoTæ ¼å¼é”™è¯¯: æ— æ³•æŒ‰'</think>\\n\\n'åˆ†å‰²æˆ–å¤„ç†ã€‚é”™è¯¯: {e}")
            report["error_counts"]["split_error"] += 1

        # ç»Ÿè®¡Tokené•¿åº¦
        report["statistics"]["output_token_lengths"].append(len(output))

        # è®°å½•æœ‰é”™è¯¯çš„æ ·æœ¬
        if errors_in_sample:
            report["error_samples"].append({"index": i, "errors": errors_in_sample})

    # --- 3. ç”Ÿæˆå¹¶æ‰“å°ä½“æ£€æŠ¥å‘Š ---
    print("\n" + "=" * 20 + " æ•°æ®é›†å¥åº·ä½“æ£€æŠ¥å‘Š " + "=" * 20)
    print(f"æ–‡ä»¶è·¯å¾„: {report['file_path']}")
    print(f"æ€»æ ·æœ¬æ•°: {report['total_samples']}")

    total_errors = sum(report["error_counts"].values())
    if total_errors == 0:
        print("\nâœ… æ­å–œï¼æœªå‘ç°ä»»ä½•æ ¼å¼æˆ–é€»è¾‘é”™è¯¯ã€‚æ•°æ®é›†çŠ¶æ€è‰¯å¥½ï¼")
    else:
        print(f"\nâŒ å‘ç° {total_errors} ä¸ªé—®é¢˜ï¼Œæ¶‰åŠ {len(report['error_samples'])} ä¸ªæ ·æœ¬ã€‚")
        print("é”™è¯¯ç±»å‹ç»Ÿè®¡:")
        for error_type, count in report["error_counts"].items():
            print(f"  - {error_type}: {count} æ¬¡")

        print("\néƒ¨åˆ†é”™è¯¯æ ·æœ¬ç¤ºä¾‹ (æœ€å¤šæ˜¾ç¤º5æ¡):")
        for err_sample in report["error_samples"][:5]:
            print(f"  - æ ·æœ¬ç´¢å¼• {err_sample['index']}: {err_sample['errors']}")

    # --- æ‰“å°ç»Ÿè®¡åˆ†æ ---
    print("\n--- ç»Ÿè®¡åˆ†æ ---")
    print(f"æ— äº‹ä»¶æ ·æœ¬æ•°: {report['statistics']['samples_with_no_events']}")

    print("\näº‹ä»¶ç±»å‹åˆ†å¸ƒ (æŒ‰é¢‘æ¬¡æ’åº):")
    event_dist = report["statistics"]["event_type_distribution"]
    if not event_dist:
        print("  æ•°æ®é›†ä¸­æœªåŒ…å«ä»»ä½•äº‹ä»¶ã€‚")
    else:
        for event_type, count in event_dist.most_common():
            print(f"  - {event_type}: {count} æ¬¡")

    print("\nOutputå­—æ®µTokené•¿åº¦åˆ†å¸ƒ:")
    if report["statistics"]["output_token_lengths"]:
        lengths_series = pd.Series(report["statistics"]["output_token_lengths"])
        print(lengths_series.describe(percentiles=[0.5, 0.75, 0.9, 0.95, 0.99]).to_string())

    print("\n" + "=" * 58)

    # å¯é€‰ï¼šå°†æŠ¥å‘Šä¿å­˜ä¸ºJSONæ–‡ä»¶
    report_save_path = os.path.splitext(file_path)[0] + "_inspection_report.json"
    # æ¸…ç†æŠ¥å‘Šä»¥ä¾¿ä¿å­˜
    del report["statistics"]["output_token_lengths"]
    with open(report_save_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=4)
    print(f"\nä¸€ä»½è¯¦ç»†çš„JSONæ ¼å¼ä½“æ£€æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_save_path}")


# --- å‘½ä»¤è¡Œå…¥å£ ---
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="ä¸€ä¸ªå…¨é¢çš„æ•°æ®é›†å¥åº·æ£€æŸ¥å·¥å…·ï¼Œç”¨äºæ ¡éªŒCoTæ ¼å¼å¾®è°ƒæ•°æ®çš„è´¨é‡ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        'file_path',
        type=str,
        help="éœ€è¦è¿›è¡Œå¥åº·æ£€æŸ¥çš„æ•°æ®é›† .json æ–‡ä»¶è·¯å¾„ã€‚"
    )
    args = parser.parse_args()

    if not os.path.isfile(args.file_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ -> {args.file_path}")
    else:
        inspect_dataset(args.file_path)